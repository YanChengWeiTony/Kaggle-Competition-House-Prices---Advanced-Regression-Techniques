# -*- coding: utf-8 -*-
"""Pred_House_Pricing_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v2QqbEVU0jxNcsZIeB3YKqEiGkesOcDm

Following:

0. following House_Pricing_9, for predicting testdata
1. https://www.kaggle.com/jesucristo/1-house-prices-solution-top-1
2. https://www.kaggle.com/itslek/blend-stack-lr-gb-0-10649-house-prices-v57/data?scriptVersionId=11189608

Kaggle: 0.11808
"""

# from google.colab import drive
# drive.mount('/content/drive')

# cd /content/drive/MyDrive/ML_2020/house-prices-advanced-regression-techniques

# ls

#import some necessary librairies
import joblib
import numpy as np  # linear algebra
import pandas as pd  #
from datetime import datetime

from scipy.stats import skew  # for some statistics
from scipy.special import boxcox1p
from scipy.stats import boxcox_normmax

from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error

import seaborn as sns

from mlxtend.regressor import StackingCVRegressor

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import sys

import os

color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)


from scipy import stats
from scipy.stats import norm, skew #for some statistics


#Now let's import and put the train and test datasets in  pandas dataframe

train = pd.read_csv(sys.argv[1])
test = pd.read_csv(sys.argv[2])
# train = pd.read_csv('train.csv')
# test = pd.read_csv('test.csv')

train.head()

test.head()

print("Train set size:", train.shape)
print("Test set size:", test.shape)

train_ID = train['Id']
test_ID = test['Id']
# Now drop the  'Id' colum since it's unnecessary for  the prediction process.
train.drop(['Id'], axis=1, inplace=True)
test.drop(['Id'], axis=1, inplace=True)

# Deleting outliers
train = train[train.GrLivArea < 4000]
train.reset_index(drop=True, inplace=True)

# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column
train["SalePrice"] = np.log1p(train["SalePrice"])
y = train.SalePrice.reset_index(drop=True)
train_features = train.drop(['SalePrice'], axis=1)
test_features = test

features = pd.concat([train_features, test_features]).reset_index(drop=True)
print(features.shape)

"""Catogory """

# Some of the non-numeric predictors are stored as numbers; we convert them into strings 
features['MSSubClass'] = features['MSSubClass'].apply(str)
features['YrSold'] = features['YrSold'].astype(str)
features['MoSold'] = features['MoSold'].astype(str)

features['Functional'] = features['Functional'].fillna('Typ')
features['Electrical'] = features['Electrical'].fillna("SBrkr")
features['KitchenQual'] = features['KitchenQual'].fillna("TA")
features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])
features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])
features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])

features["PoolQC"] = features["PoolQC"].fillna("None")

for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    features[col] = features[col].fillna(0)
for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:
    features[col] = features[col].fillna('None')
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    features[col] = features[col].fillna('None')

features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))

objects = []
for i in features.columns:
    if features[i].dtype == object:
        objects.append(i)

features.update(features[objects].fillna('None'))

features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

# Filling in the rest of the NA's

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes:
        numerics.append(i)
features.update(features[numerics].fillna(0))

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics2 = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes:
        numerics2.append(i)

skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)

high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index

for i in skew_index:
    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))

features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)

features['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']
features['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']

features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +
                                 features['1stFlrSF'] + features['2ndFlrSF'])

features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +
                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))

features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +
                              features['EnclosedPorch'] + features['ScreenPorch'] +
                              features['WoodDeckSF'])

# simplified features
features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)

print(features.shape)
final_features = pd.get_dummies(features).reset_index(drop=True)
print(final_features.shape)

X = final_features.iloc[:len(y), :]
X_sub = final_features.iloc[len(X):, :]

print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)

outliers = [30, 88, 462, 631, 1322]
X = X.drop(X.index[outliers])
y = y.drop(y.index[outliers])

overfit = []
for i in X.columns:
    counts = X[i].value_counts()
    zeros = counts.iloc[0]
    if zeros / len(X) * 100 > 99.94:
        overfit.append(i)

overfit = list(overfit)
overfit.append('MSZoning_C (all)')

print(overfit)

X = X.drop(overfit, axis=1).copy()
X_sub = X_sub.drop(overfit, axis=1).copy()

print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)

"""# ML

Load Model
"""

import pickle


kfolds = KFold(n_splits=10, shuffle=True, random_state=42)

# rmsle
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))


# build our model scoring function
def cv_rmse(model, X=X):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=kfolds))
    return (rmse)

# load the model from disk
import xgboost as xgb
mydir = sys.argv[4]
# mydir = "House_Pricing_9_mdl/"
stack_gen_model = pickle.load(open(mydir + "stack_gen_model.sav", 'rb'))
elastic_model_full_data = pickle.load(open(mydir +  "elastic_model.sav", 'rb'))
lasso_model_full_data = pickle.load(open(mydir +  "lasso_model.sav", 'rb'))
ridge_model_full_data = pickle.load(open(mydir +  "ridge_model.sav", 'rb'))
svr_model_full_data = pickle.load(open(mydir +  "svr_model.sav", 'rb'))
gbr_model_full_data = pickle.load(open(mydir +  "gbr_model.sav", 'rb'))
xgb_model_full_data = pickle.load(open(mydir +  "xgb_model.sav", 'rb'))
# xgb_model_full_data = xgb.Booster({'nthread': 4})  # init model
# xgb_model_full_data.load_model(mydir + 'xgb_model.model')  # load data
lgb_model_full_data = pickle.load(open(mydir +  "lgb_model.sav", 'rb'))

"""Run results"""

# def blend_models_predict(X):
#     return ((0.04 * elastic_model_full_data.predict(X)) + \
#             (0.08 * lasso_model_full_data.predict(X)) + \
#             (0.08 * ridge_model_full_data.predict(X)) + \
#             (0.04 * svr_model_full_data.predict(X)) + \
#             (0.04 * gbr_model_full_data.predict(X)) + \
#             (0.04 * xgb_model_full_data.predict(X)) + \
#             (0.08 * lgb_model_full_data.predict(X)) + \
#             (0.6 * stack_gen_model.predict(np.array(X))))
            
def blend_models_predict(X):
    return ((0.04 * elastic_model_full_data.predict(X)) + \
            (0.04 * lasso_model_full_data.predict(X)) + \
            (0.04 * ridge_model_full_data.predict(X)) + \
            (0.04 * svr_model_full_data.predict(X)) + \
            (0.04 * gbr_model_full_data.predict(X)) + \
            (0.06 * xgb_model_full_data.predict( X )) + \
            (0.04 * lgb_model_full_data.predict(X)) + \
            (0.7 * stack_gen_model.predict(np.array(X))))
    

print('RMSLE score on train data:')
print(rmsle(y, blend_models_predict(X)))

print('Predict submission', datetime.now(),)
submission = pd.read_csv("sample_submission.csv")
submission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))

output_name = sys.argv[3]
submission.to_csv(output_name,index=False)
# submission.to_csv("submission.csv", index=False)
# submission.to_csv("submission_9.csv", index=False)

