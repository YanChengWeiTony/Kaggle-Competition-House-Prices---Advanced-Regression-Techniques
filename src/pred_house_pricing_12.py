# -*- coding: utf-8 -*-
"""Pred_House_Pricing_12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lkFujXGE7dJL333eVBuzGl7Pawhlha5a

Following:

0. House_Pricing_12, for predicting testdata
1. https://www.kaggle.com/hemingwei/top-2-from-laurenstc-on-house-price-prediction
2. https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf

Kaggle: 0.11898
"""

# from google.colab import drive
# drive.mount('/content/drive')

# cd /content/drive/MyDrive/ML_2020/house-prices-advanced-regression-techniques

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('seaborn')
from scipy.stats import norm, skew
import numpy as np
import seaborn as sns
import sys

#Data reading

train = pd.read_csv(sys.argv[1])
test = pd.read_csv(sys.argv[2])
# train = pd.read_csv("train.csv")
# test = pd.read_csv("test.csv")
# print("Train set size:", train.shape)
# print("Test set size:", test.shape)

# plt.scatter(train.GrLivArea, train.SalePrice)
# plt.show()

train = train[train.GrLivArea < 4500]
# plt.scatter(train.GrLivArea, train.SalePrice)

train.drop(['Id'], axis=1, inplace=True)
test.drop(['Id'], axis=1, inplace=True)

df = pd.concat([train.SalePrice, np.log(train.SalePrice + 1).rename('LogSalePrice')], axis=1, names=['SalePrice', 'LogSalePrice'])
df.head()

# plt.subplot(1, 2, 1)
# sns.distplot(train.SalePrice, kde=False, fit = norm)

# plt.subplot(1, 2, 2)
# sns.distplot(np.log(train.SalePrice + 1), kde=False, fit = norm)
# plt.xlabel('Log SalePrice')

train.SalePrice = np.log1p(train.SalePrice)

y = train.SalePrice.reset_index(drop=True)
train_features = train.drop(['SalePrice'], axis=1)
test_features = test

features = pd.concat([train_features, test_features]).reset_index(drop=True)
features.shape

nulls = np.sum(features.isnull())

nullcols = nulls.loc[(nulls != 0)]
dtypes = features.dtypes
dtypes2 = dtypes.loc[(nulls != 0)]
info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)
# print(info)
# print("There are", len(nullcols), "columns with missing values")

features['Functional'] = features['Functional'].fillna('Typ')
features['Electrical'] = features['Electrical'].fillna("SBrkr")
features['KitchenQual'] = features['KitchenQual'].fillna("TA")

features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])
features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])

features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])

pd.set_option('max_columns', None)
features[(features['PoolArea'] > 0 )& features['PoolQC'].isnull()]

features.loc[2418, 'PoolQC'] = 'Fa'
features.loc[2501, 'PoolQC'] = 'Gd'
features.loc[2597, 'PoolQC'] = 'Fa'

pd.set_option('max_columns', None)
features[(features['GarageType'] == 'Detchd') & features['GarageYrBlt'].isnull()]

features.loc[2124, 'GarageYrBlt'] = features['GarageYrBlt'].median()
features.loc[2574, 'GarageYrBlt'] = features['GarageYrBlt'].median()

features.loc[2124, 'GarageFinish'] = features['GarageFinish'].mode()[0]
features.loc[2574, 'GarageFinish'] = features['GarageFinish'].mode()[0]

features.loc[2574, 'GarageCars'] = features['GarageCars'].median()

features.loc[2124, 'GarageArea'] = features['GarageArea'].median()
features.loc[2574, 'GarageArea'] = features['GarageArea'].median()

features.loc[2124, 'GarageQual'] = features['GarageQual'].mode()[0]
features.loc[2574, 'GarageQual'] = features['GarageQual'].mode()[0]

features.loc[2124, 'GarageCond'] = features['GarageCond'].mode()[0]
features.loc[2574, 'GarageCond'] = features['GarageCond'].mode()[0]

basement_columns = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
                   'BsmtFinType2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',
                   'TotalBsmtSF']

tempdf = features[basement_columns]
tempdfnulls = tempdf[tempdf.isnull().any(axis=1)]

#now select just the rows that have less then 5 NA's, 
# meaning there is incongruency in the row.
tempdfnulls[(tempdfnulls.isnull()).sum(axis=1) < 5]

features.loc[332, 'BsmtFinType2'] = 'ALQ' #since smaller than SF1
features.loc[947, 'BsmtExposure'] = 'No' 
features.loc[1485, 'BsmtExposure'] = 'No'
features.loc[2038, 'BsmtCond'] = 'TA'
features.loc[2183, 'BsmtCond'] = 'TA'
features.loc[2215, 'BsmtQual'] = 'Po' #v small basement so let's do Poor.
features.loc[2216, 'BsmtQual'] = 'Fa' #similar but a bit bigger.
features.loc[2346, 'BsmtExposure'] = 'No' #unfinished bsmt so prob not.
features.loc[2522, 'BsmtCond'] = 'Gd' #cause ALQ for bsmtfintype1

subclass_group = features.groupby('MSSubClass')
Zoning_modes = subclass_group['MSZoning'].apply(lambda x : x.mode()[0])
Zoning_modes

features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))

objects = []
for i in features.columns:
    if features[i].dtype == object:
        objects.append(i)

features.update(features[objects].fillna('None'))

nulls = np.sum(features.isnull())
nullcols = nulls.loc[(nulls != 0)]
dtypes = features.dtypes
dtypes2 = dtypes.loc[(nulls != 0)]
info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)
# print(info)
# print("There are", len(nullcols), "columns with missing values")

neighborhood_group = features.groupby('Neighborhood')
lot_medians = neighborhood_group['LotFrontage'].median()
lot_medians

features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

pd.set_option('max_columns', None)
features[(features['GarageYrBlt'].isnull()) & features['GarageArea'] > 0]

pd.set_option('max_columns', None)
features[(features['MasVnrArea'].isnull())]

#Filling in the rest of the NA's

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes: 
        numerics.append(i)
        
features.update(features[numerics].fillna(0))

nulls = np.sum(features.isnull())
nullcols = nulls.loc[(nulls != 0)]
dtypes = features.dtypes
dtypes2 = dtypes.loc[(nulls != 0)]
info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)
# print(info)
# print("There are", len(nullcols), "columns with missing values")

features.describe()

features[features['GarageYrBlt'] == 2207]

features.loc[2590, 'GarageYrBlt'] = 2007

#factors = ['MSSubClass', 'MoSold']
factors = ['MSSubClass']
 


for i in factors:
    # print(features[i].dtype)
    features.update(features[i].astype('str'))

"""Skewness"""

from scipy.stats import skew

numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics2 = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes: 
        numerics2.append(i)

skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)
skews = pd.DataFrame({'skew':skew_features})
skews

from scipy.special import boxcox1p
from scipy.stats import boxcox_normmax

high_skew = skew_features[skew_features > 0.5]
high_skew = high_skew
skew_index = high_skew.index

for i in skew_index:
    features[i]= boxcox1p(features[i], boxcox_normmax(features[i]+1))

        
skew_features2 = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)
skews2 = pd.DataFrame({'skew':skew_features2})
skews2

objects3 = []
for i in features.columns:
    if features[i].dtype == object:
        objects3.append(i)

# print("Training Set incomplete cases")

sums_features = features[objects3].apply(lambda x: len(np.unique(x)))
sums_features.sort_values(ascending=False)

# print(features['Street'].value_counts())
# print('-----')
# print(features['Utilities'].value_counts())
# print('-----')
# print(features['CentralAir'].value_counts())
# print('-----')
# print(features['PavedDrive'].value_counts())

#features = features.drop(['Utilities'], axis=1)
features = features.drop(['Utilities', 'Street'], axis=1)

features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +
                                 features['1stFlrSF'] + features['2ndFlrSF'])

features['Total_Bathrooms'] = (features['FullBath'] + (0.5*features['HalfBath']) + 
                               features['BsmtFullBath'] + (0.5*features['BsmtHalfBath']))

features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +
                              features['EnclosedPorch'] + features['ScreenPorch'] +
                             features['WoodDeckSF'])


#simplified features
features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)

features.shape

final_features = pd.get_dummies(features).reset_index(drop=True)
final_features.shape

y.shape

X = final_features.iloc[:len(y),:]
testing_features = final_features.iloc[len(X):,:]

# print(X.shape)
# print(testing_features.shape)

outliers = [30, 88, 462, 631, 1322]

X = X.drop(X.index[outliers])
y = y.drop(y.index[outliers])

overfit = []
for i in X.columns:
    counts = X[i].value_counts()
    zeros = counts.iloc[0]
    if zeros / len(X) * 100 >99.94:
        overfit.append(i)

overfit = list(overfit)
overfit

overfit.append('MSZoning_C (all)')

overfit

X.drop(overfit,axis=1,inplace=True)
testing_features.drop(overfit,axis=1,inplace=True)

# print(X.shape)
# print(testing_features.shape)

"""# ML

Load the model
"""

import pickle
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import make_pipeline


kfolds = KFold(n_splits=10, shuffle=True, random_state=42)

# rmsle
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))


# build our model scoring function
def cv_rmse(model, X=X):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=kfolds))
    return (rmse)

# load the model from disk
mydir = sys.argv[4]
# mydir = "House_Pricing_12_mdl/"
stack_gen_model = pickle.load(open(mydir + "stack_gen_model.sav", 'rb'))
elastic_model3 = pickle.load(open(mydir +  "elastic_model.sav", 'rb'))
lasso_model2 = pickle.load(open(mydir +  "lasso_model.sav", 'rb'))
ridge_model2 = pickle.load(open(mydir +  "ridge_model.sav", 'rb'))
svr_fit = pickle.load(open(mydir +  "svr_model.sav", 'rb'))
# gbr_model_full_data = pickle.load(open(mydir +  "gbr_model.sav", 'rb'))
xgb_fit = pickle.load(open(mydir +  "xgb_model.sav", 'rb'))
lgbm_fit = pickle.load(open(mydir +  "lgb_model.sav", 'rb'))

em_preds = elastic_model3.predict(testing_features)
lasso_preds = lasso_model2.predict(testing_features)
ridge_preds = ridge_model2.predict(testing_features)
stack_gen_preds = stack_gen_model.predict(testing_features)
xgb_preds = xgb_fit.predict(testing_features)
svr_preds = svr_fit.predict(testing_features)
lgbm_preds = lgbm_fit.predict(testing_features)


# stack_preds = ((0.2*em_preds) + (0.1*lasso_preds) + (0.1*ridge_preds) + 
#                (0.2*xgb_preds) + (0.1*lgbm_preds) + (0.3*stack_gen_preds))

stack_preds = ((0.16*em_preds) + (0.08*lasso_preds) + (0.08*ridge_preds) + 
               (0.16*xgb_preds) + (0.08*lgbm_preds) + (0.44 *stack_gen_preds))

submission = pd.read_csv("sample_submission.csv")
submission.iloc[:,1] = np.expm1(stack_preds)
output_name = sys.argv[3]
submission.to_csv(output_name,index=False)
# submission.to_csv("submission.csv", index=False)
# submission.to_csv("submission_12.csv", index=False)

